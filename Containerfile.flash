# Multi-stage build for flash-attention from source
ARG CUDA_VERSION=12.4.1
ARG PYTHON_VERSION=3.12

# Stage 1: Build flash-attention
FROM nvidia/cuda:${CUDA_VERSION}-cudnn-devel-ubuntu22.04 AS flash-attn-builder

ARG PYTHON_VERSION=3.12

# Install Python and build dependencies
RUN DEBIAN_FRONTEND=noninteractive apt-get update && apt-get install -y \
    software-properties-common \
    git \
    && add-apt-repository ppa:deadsnakes/ppa \
    && DEBIAN_FRONTEND=noninteractive apt-get update && apt-get install -y \
    python${PYTHON_VERSION} \
    python${PYTHON_VERSION}-dev \
    python${PYTHON_VERSION}-venv \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

# Set Python 3.12 as default
RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \
    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION}

# Install pip for Python 3.12
RUN curl -sS https://bootstrap.pypa.io/get-pip.py | python3

# Install PyTorch with CUDA support
RUN pip3 install torch==2.6.0+cu124 --index-url https://download.pytorch.org/whl/cu124

# Clone and build flash-attention
ENV CUDA_HOME=/usr/local/cuda
WORKDIR /build
RUN git clone https://github.com/Dao-AILab/flash-attention.git && \
    cd flash-attention && \
    pip3 install flash-attn . --no-build-isolation && \
    # Also create wheels for later use
    pip3 wheel flash-attn . --no-build-isolation -w /wheels

# Stage 2: Runtime image with CUDA runtime
FROM nvidia/cuda:${CUDA_VERSION}-cudnn-runtime-ubuntu22.04

ARG PYTHON_VERSION=3.12

# Install Python and runtime dependencies
RUN DEBIAN_FRONTEND=noninteractive apt-get update && apt-get install -y \
    software-properties-common \
    && add-apt-repository ppa:deadsnakes/ppa \
    && DEBIAN_FRONTEND=noninteractive apt-get update && apt-get install -y \
    python${PYTHON_VERSION} \
    python${PYTHON_VERSION}-dev \
    python${PYTHON_VERSION}-venv \
    python3-pip \
    git \
    tesseract-ocr \
    tesseract-ocr-eng \
    tesseract-ocr-osd \
    libtesseract-dev \
    libleptonica-dev \
    libgl1-mesa-glx \
    && rm -rf /var/lib/apt/lists/*

# Set Python 3.12 as default
RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \
    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION}

# Install pip for Python 3.12
RUN curl -sS https://bootstrap.pypa.io/get-pip.py | python3

# Create app user
RUN useradd -m -u 1001 -s /bin/bash appuser

WORKDIR /opt/app-root/src

# Copy flash-attention wheel from builder
COPY --from=flash-attn-builder /wheels/*.whl /tmp/

# Install uv
RUN curl -LsSf https://astral.sh/uv/0.7.19/install.sh | sh && \
    mv /root/.local/bin/uv /usr/local/bin/

# Set environment variables
ENV OMP_NUM_THREADS=4 \
    LANG=en_US.UTF-8 \
    LC_ALL=en_US.UTF-8 \
    PYTHONIOENCODING=utf-8 \
    TRANSFORMERS_VERBOSITY=info \
    UV_COMPILE_BYTECODE=1 \
    UV_LINK_MODE=copy \
    UV_PROJECT_ENVIRONMENT=/opt/app-root \
    DOCLING_SERVE_ARTIFACTS_PATH=/opt/app-root/src/.cache/docling/models \
    TESSDATA_PREFIX=/usr/share/tesseract-ocr/5/tessdata/

# Copy project files
COPY --chown=1001:0 pyproject.toml uv.lock ./

# Install dependencies (excluding flash-attn since we'll install from wheel)
RUN uv sync --frozen --no-install-project --no-dev --all-extras --no-extra flash-attn

# Install the pre-built flash-attention wheel
RUN pip3 install /tmp/flash-attn*.whl && rm -f /tmp/*.whl

# Download models
ARG MODELS_LIST="layout tableformer picture_classifier easyocr smoldocling qwenvl"
RUN HF_HUB_DOWNLOAD_TIMEOUT="90" \
    HF_HUB_ETAG_TIMEOUT="90" \
    docling-tools models download -o "${DOCLING_SERVE_ARTIFACTS_PATH}" ${MODELS_LIST}

# Copy application
COPY --chown=1001:0 ./docling_serve ./docling_serve

# Install the application
RUN uv sync --frozen --no-dev --all-extras

# Fix permissions
RUN chown -R 1001:0 /opt/app-root && \
    chmod -R g=u /opt/app-root

USER 1001

EXPOSE 5001

CMD ["docling-serve", "run"]